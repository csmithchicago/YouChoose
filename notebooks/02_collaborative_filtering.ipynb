{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/interim/small_10000_orders_weighted_adjacency_matrix.csv\")\n",
    "# df = df.sample(500)\n",
    "df[\"rating\"] = df.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Most Purchased Products:\n",
      "       product_id  weight\n",
      "3169        13176    1307\n",
      "5904        24852    1305\n",
      "5228        21903     963\n",
      "5032        21137     861\n",
      "11221       47209     785\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_grouped = df[[\"product_id\", \"weight\"]].groupby([\"product_id\"]).sum()\n",
    "df_grouped.reset_index(inplace=True)\n",
    "print(\n",
    "    f\"5 Most Purchased Products:\\n{df_grouped.sort_values(by='weight', ascending=False).head(5)}\"\n",
    ")\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "ax1.hist(df.weight, bins=50)\n",
    "ax1.set_yscale(\"log\")\n",
    "ax1.set_title(f\"Average Number of Purchases Per Order: {df.weight.mean():.2f}\")\n",
    "\n",
    "ax2.hist(df_grouped.weight, bins=50)\n",
    "ax2.set_title(\n",
    "    f\"Average Number of Times Each Product is Ordered: {df_grouped.weight.mean():.2f}\"\n",
    ")\n",
    "ax2.set_yscale(\"log\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the models we will want to use a numeric index rather than the product_id and user_id so that we can easily match the embedding vector with the user or product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prod_sets = product_sets(df)\n",
    "\n",
    "# id_u_dict = list_to_indexed_dict(df.user_id)\n",
    "# id_p_dict = list_to_indexed_dict(df.product_id)\n",
    "\n",
    "# u_dict = {key: val for val, key in id_u_dict.items()}\n",
    "# p_dict = {key: val for val, key in id_p_dict.items()}\n",
    "# w_dict = {val: 1 for val in df.weight.unique()}\n",
    "\n",
    "# num_prods = len(p_dict)\n",
    "# num_negs = 5\n",
    "# idx = 5\n",
    "\n",
    "# df_s = df.sample(20)\n",
    "# user_ids = df_s[\"user_id\"].iloc[10]\n",
    "\n",
    "\n",
    "# def negative_sampling(num_negs, user_ids, prod_sets, p_dict, u_dict):\n",
    "#     \"\"\"\n",
    "#     Draw negative samples for each of the positive interactions.\n",
    "#     \"\"\"\n",
    "#     if isinstance(user_ids, np.int64):\n",
    "#         user_ids = [user_ids]\n",
    "\n",
    "#     u_list = []\n",
    "#     p_list = []\n",
    "#     r_list = num_negs * len(user_ids) * [0.]\n",
    "\n",
    "#     for u_id in user_ids:\n",
    "#         neg_set = set(p_dict.keys()) - prod_sets[u_id]\n",
    "#         neg_v = np.random.choice(tuple(neg_set), num_negs) \n",
    "\n",
    "#         for p_id in neg_v:\n",
    "#             u_list.append(u_dict[u_id])\n",
    "#             p_list.append(p_dict[p_id])\n",
    "\n",
    "#     return (torch.tensor(u_list), torch.tensor(p_list), torch.tensor(r_list))\n",
    "\n",
    "# u_samp = torch.from_numpy(pd.Series(df_s[\"user_id\"].iloc[:10]).map(u_dict).to_numpy())\n",
    "# p_samp = torch.from_numpy(pd.Series(df_s[\"product_id\"].iloc[:10]).map(p_dict).to_numpy())\n",
    "# w_samp = torch.from_numpy(pd.Series(df_s[\"rating\"].iloc[:10]).map(w_dict).to_numpy()).float()\n",
    "\n",
    "# negative_sampling(num_negs, user_ids, prod_sets, p_dict, u_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_sets(df):\n",
    "    \"\"\"\n",
    "    Generate sets for each user containing products that \n",
    "    they have previously purchased.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas DataFrame): Dataframe containing user_id and\n",
    "            product_id columns.\n",
    "    Return:\n",
    "        dictionary with user_id as keys and the product sets as values.\n",
    "    \"\"\"\n",
    "    df_g = (\n",
    "        df[[\"user_id\", \"product_id\"]]\n",
    "        .groupby([\"user_id\"])[\"product_id\"]\n",
    "        .agg(lambda x: set([val for val in x]))\n",
    "    )\n",
    "    df_g = df_g.reset_index()\n",
    "    df_g.columns = [\"user_id\", \"product_list\"]\n",
    "\n",
    "    return df_g.set_index(\"user_id\").to_dict()[\"product_list\"]\n",
    "\n",
    "\n",
    "def list_to_indexed_dict(list_):\n",
    "    \"\"\"\n",
    "    Assign id to distinct list elements and return \n",
    "    the id -> element mapping as a dictionary.\n",
    "    \"\"\"\n",
    "    return dict(enumerate(sorted(set(list_))))\n",
    "\n",
    "\n",
    "class RatingsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    User, product, ratings dataset. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataframe,\n",
    "        product_dict,\n",
    "        user_dict,\n",
    "        dev=torch.device(\"cpu\"),\n",
    "        reweighting=dict(),\n",
    "        num_negs=0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Dataframe containing ratings.\n",
    "            product_dict (dict): Dictionary mapping product ids to indices.\n",
    "            user_dict (dict): Dictionary mapping user ids to indices.\n",
    "            reweighting (dict, optional): Dictionary mapping ratings to new values.\n",
    "            dev (torch device): Hardware that the model should run on.\n",
    "        \"\"\"\n",
    "        super(RatingsDataset, self).__init__()\n",
    "\n",
    "        if num_negs < 0:\n",
    "            raise ValueError(\"The number of negative samples must be positive.\")\n",
    "\n",
    "        self.df = dataframe\n",
    "        self.prod_sets = product_sets(dataframe)\n",
    "        self.p_dict = product_dict\n",
    "        self.u_dict = user_dict\n",
    "        self.w_dict = reweighting\n",
    "        self.num_prods = len(product_dict)\n",
    "        self.num_users = len(user_dict)\n",
    "        self.dev = dev\n",
    "        self.num_negs = num_negs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get and return Tensor for item, user, ratings triplet\n",
    "        \"\"\"\n",
    "        user_ids = self.df[\"user_id\"].iloc[idx]\n",
    "        item = self.transform(self.df[\"product_id\"].iloc[idx], self.p_dict)\n",
    "        user = self.transform(user_ids, self.u_dict)\n",
    "        rating = self.transform(self.df[\"rating\"].iloc[idx], self.w_dict).float()\n",
    "\n",
    "        if self.num_negs:\n",
    "            neg_i, users, neg_r = self.negative_sampling(user_ids)\n",
    "\n",
    "            item = torch.cat((item, neg_i))\n",
    "            user = torch.cat((user, users))\n",
    "            rating = torch.cat((rating, neg_r))\n",
    "\n",
    "        return (item, user, rating)\n",
    "\n",
    "    def transform(self, df_rows, mapping):\n",
    "        \"\"\"\n",
    "        Replace dataframe with index values and convert to torch.Tensor\n",
    "        \"\"\"\n",
    "        transformed = pd.Series(df_rows).replace(mapping).to_numpy()\n",
    "\n",
    "        return torch.from_numpy(transformed).to(self.dev)\n",
    "\n",
    "    def negative_sampling(self, user_ids):\n",
    "        \"\"\"\n",
    "        For each user interaction randomly sample products that they\n",
    "        have not previously purchased.\n",
    "        \"\"\"\n",
    "        if isinstance(user_ids, np.int64):\n",
    "            user_ids = [user_ids]\n",
    "\n",
    "        u_list = []\n",
    "        p_list = []\n",
    "        r_list = self.num_negs * len(user_ids) * [0.0]\n",
    "\n",
    "        for u_id in user_ids:\n",
    "            neg_set = set(self.p_dict.keys()) - self.prod_sets[u_id]\n",
    "            neg_v = np.random.choice(tuple(neg_set), self.num_negs)\n",
    "\n",
    "            for p_id in neg_v:\n",
    "                u_list.append(self.u_dict[u_id])\n",
    "                p_list.append(self.p_dict[p_id])\n",
    "\n",
    "        return (torch.tensor(p_list), torch.tensor(u_list), torch.tensor(r_list))\n",
    "\n",
    "\n",
    "def dataframe_split(df, train_frac=0.80, test_frac=0.10):\n",
    "    \"\"\"\n",
    "    Split dataframe into training, testing, and validation sets.\n",
    "    \"\"\"\n",
    "    train_df = df.sample(frac=train_frac, random_state=23)\n",
    "    test_df = df.drop(train_df.index).sample(int(test_frac * len(df)), random_state=23)\n",
    "    # All samples not in test or train sets are used for validation\n",
    "    val_df = df.drop(pd.concat([train_df, test_df], axis=0).index)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "class MatrixFactorization(torch.nn.Module):\n",
    "    \"\"\"Matrix factorization using pytorch.\"\"\"\n",
    "\n",
    "    def __init__(self, n_users, n_products, n_factors=20):\n",
    "        \"\"\"\n",
    "        Initalize the user and product embedding vectors in latent space.\n",
    "        \n",
    "        Args:\n",
    "            n_users (int): Number of users with prior purchases.\n",
    "            n_products (int): Total number of products purchased.\n",
    "            n_factors (integer, optional): Dimension of the latent embedding space.\n",
    "        \"\"\"\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.product_factors = nn.Embedding(n_products, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.product_bias = nn.Embedding(n_products, 1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        \"\"\" \n",
    "        Matrix multiplication between user and product \n",
    "        embedding vectors.\n",
    "        \"\"\"\n",
    "        prediction = self.user_bias(user).squeeze() + self.product_bias(item).squeeze()\n",
    "        prediction += (\n",
    "            ((self.user_factors(user)) * (self.product_factors(item))).sum(2).squeeze()\n",
    "        )\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def prediction(self, user, item):\n",
    "        \"\"\"\n",
    "        Use product and user embedding vectors to calculate\n",
    "        a probability for positive interaction.\n",
    "        \"\"\"\n",
    "        activation = nn.Sigmoid()\n",
    "        prediction = self.user_bias(user).squeeze() + self.product_bias(item).squeeze()\n",
    "        prediction += (\n",
    "            ((self.user_factors(user)) * (self.product_factors(item))).sum(2).squeeze()\n",
    "        )\n",
    "        predict_pos = activation(prediction)\n",
    "        predict_neg = 1 - predict_pos\n",
    "\n",
    "        return torch.stack((predict_neg, predict_pos)).argmax(0).float()\n",
    "\n",
    "    def compute_loss(self, loss_fn, item, user, rating):\n",
    "        \"\"\"Calculate the loss of the predicted ratings.\"\"\"\n",
    "        return loss_fn(self.forward(user, item), rating.float().squeeze())\n",
    "\n",
    "    def compute_accuracy(self, data_loader):\n",
    "        \"\"\"\n",
    "        Compute the accuracy of our predictions against the true ratings.\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for item, user, true_rating in data_loader:\n",
    "                predicted = self.prediction(user, item)\n",
    "                total += predicted.numel()\n",
    "                correct += (predicted == true_rating).sum().item()\n",
    "\n",
    "        return total, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are only taking 500 interactions\n",
    "df_use = df.sample(100)\n",
    "num_negs = 15\n",
    "\n",
    "train_df, val_df, test_df = dataframe_split(\n",
    "    df_use\n",
    ")\n",
    "index_to_product_dict = list_to_indexed_dict(df_use.product_id)\n",
    "index_to_user_dict = list_to_indexed_dict(df_use.user_id)\n",
    "\n",
    "product_to_index_dict = {key: value for value, key in index_to_product_dict.items()}\n",
    "user_to_index_dict = {key: value for value, key in index_to_user_dict.items()}\n",
    "reweight_dict = {val: 1.0 for val in df_use.weight.unique()}\n",
    "\n",
    "n_users, n_products = len(user_to_index_dict), len(product_to_index_dict)\n",
    "\n",
    "train_data = RatingsDataset(\n",
    "    train_df,\n",
    "    product_to_index_dict,\n",
    "    user_to_index_dict,\n",
    "    reweighting=reweight_dict,\n",
    "    dev=dev,\n",
    "    num_negs=num_negs,\n",
    ")\n",
    "val_data = RatingsDataset(\n",
    "    val_df,\n",
    "    product_to_index_dict,\n",
    "    user_to_index_dict,\n",
    "    reweighting=reweight_dict,\n",
    "    dev=dev,\n",
    "    num_negs=num_negs,\n",
    ")\n",
    "test_data = RatingsDataset(\n",
    "    test_df,\n",
    "    product_to_index_dict,\n",
    "    user_to_index_dict,\n",
    "    reweighting=reweight_dict,\n",
    "    dev=dev,\n",
    "    num_negs=num_negs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 50\n",
    "epochs = 100\n",
    "print_every = 5\n",
    "embedding_dim = 10\n",
    "l2_penalty = 0.001\n",
    "learning_rate = 1e-6\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=bs)\n",
    "test_loader = DataLoader(test_data, batch_size=bs)\n",
    "\n",
    "model = MatrixFactorization(n_users, n_products, n_factors=embedding_dim)\n",
    "model.to(dev)\n",
    "# loss_fn = nn.MSELoss()\n",
    "model.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model.optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=l2_penalty\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in trange(epochs):\n",
    "    train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    train_loss = train(model, train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = [\n",
    "            model.compute_loss(loss_fn, item, user, rating).item()\n",
    "            for item, user, rating in val_loader\n",
    "        ]\n",
    "    val_loss = np.sum(val_loss)\n",
    "    val_total, val_correct = model.compute_accuracy(val_loader)\n",
    "\n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(\n",
    "            f\"epoch #{epoch + 1}, training loss: {train_loss:0.3f}, \"\n",
    "            f\"validation loss: {val_loss:0.3f}, \"\n",
    "            f\"validation accuracy: {(100 * val_correct / val_total):.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "users_items = sigmoid(\n",
    "    model.user_factors.weight @ model.product_factors.weight.transpose(0, 1)\n",
    ")\n",
    "values = users_items.detach().numpy()\n",
    "\n",
    "plt.hist(values.ravel(), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    \"\"\"\n",
    "    Train the model on the data generated by the dataloader.\n",
    "    \"\"\"\n",
    "    train_loss = 0\n",
    "    for item, user, rating in train_loader:\n",
    "        model.optimizer.zero_grad()\n",
    "        loss = model.compute_loss(model.loss_fn, item, user, rating)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "\n",
    "#     return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example comes directly from the documentation on torch.multiprocessing\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "bs = 50\n",
    "epochs = 100\n",
    "print_every = 5\n",
    "embedding_dim = 10\n",
    "l2_penalty = 0.001\n",
    "learning_rate = 1e-6\n",
    "num_processes = 48\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=bs)\n",
    "test_loader = DataLoader(test_data, batch_size=bs)\n",
    "\n",
    "model = MatrixFactorization(n_users, n_products, n_factors=embedding_dim)\n",
    "model.to(dev)\n",
    "\n",
    "# loss_fn = nn.MSELoss()\n",
    "model.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model.optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=l2_penalty\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4333e+00, -8.9963e-01, -1.9330e-02, -1.7814e+00,  2.0324e-02,\n",
       "         -4.1041e-01,  1.4695e+00, -2.3004e+00, -5.5006e-01,  9.6585e-01],\n",
       "        [-1.4763e+00, -4.5092e-01,  9.0537e-01, -1.7011e+00,  1.7471e+00,\n",
       "          1.2400e+00,  7.6806e-01, -7.3178e-01,  1.9154e-01, -1.6154e+00],\n",
       "        [ 3.8457e-01, -6.9640e-01,  4.4247e-01,  8.9063e-01,  7.2175e-01,\n",
       "          1.5651e-01,  5.6657e-01, -6.5262e-01,  5.3221e-01, -1.0787e+00],\n",
       "        [-9.5290e-01,  3.6351e-02,  8.9709e-01,  2.3399e-01,  6.7542e-01,\n",
       "         -8.4097e-01,  9.9748e-01, -1.3285e+00, -3.4416e-02,  5.0887e-01],\n",
       "        [ 5.9784e-01, -9.0697e-01, -7.1282e-01, -2.4948e-01, -6.4971e-01,\n",
       "          8.7206e-01, -5.8542e-01, -3.2919e-01, -3.4160e-01,  1.3576e-01],\n",
       "        [-1.9971e-01,  1.5334e+00,  1.3532e+00,  1.1532e+00, -1.7517e+00,\n",
       "         -8.9848e-01, -5.7436e-01,  2.4249e-01,  1.1253e+00,  1.1115e+00],\n",
       "        [-1.9362e-01,  1.7086e+00,  2.7931e-01,  5.5493e-01, -9.6455e-01,\n",
       "         -1.7386e+00,  2.4911e-01, -1.5715e+00,  3.8965e-01,  1.3052e+00],\n",
       "        [-9.3871e-01,  1.5376e-01, -1.0461e+00,  9.5612e-01, -1.6406e+00,\n",
       "         -3.6505e-01,  3.5504e-01,  1.7138e+00, -5.2552e-01,  5.3002e-02],\n",
       "        [-2.3685e+00, -1.7228e+00, -5.6088e-01,  2.4156e-01,  3.3708e-02,\n",
       "         -6.8205e-01,  5.4853e-01,  2.3913e-01, -3.7969e-02, -2.3545e-01],\n",
       "        [-2.2254e-01, -8.5967e-01,  2.8247e-01, -1.0681e+00, -8.9280e-01,\n",
       "         -8.1529e-01, -1.4039e-01,  2.2843e-01, -2.2449e+00, -1.9916e+00],\n",
       "        [ 2.7216e-01, -1.0698e+00, -1.1813e+00, -1.6923e+00,  1.2578e+00,\n",
       "          1.6030e+00, -4.9662e-01,  1.2040e-01, -1.0337e+00,  4.3306e-01],\n",
       "        [-2.7639e-01,  3.8737e-01,  9.5247e-02, -1.1213e+00, -7.0410e-01,\n",
       "          2.9937e-01,  1.4210e+00, -4.2727e-01, -2.2150e+00, -1.2833e+00],\n",
       "        [-1.3831e-01,  1.6157e+00,  7.6762e-01,  2.2031e-01,  5.2692e-01,\n",
       "          5.8650e-01,  3.7986e-01,  2.3556e-01, -2.1529e-01,  9.6105e-01],\n",
       "        [-6.1858e-01,  1.6063e+00, -2.7682e-01,  5.5596e-01, -3.5311e-02,\n",
       "          1.0390e+00,  1.2079e-01,  2.3063e+00,  1.4701e+00, -1.9631e+00],\n",
       "        [-1.0903e+00,  1.9322e-04,  5.3116e-02,  1.9153e+00,  1.0844e+00,\n",
       "          4.8793e-01,  1.1994e+00,  5.9839e-02,  9.3380e-01,  6.5793e-01],\n",
       "        [ 1.4146e+00, -7.4532e-01, -1.9150e-01,  1.4040e-01,  3.6602e-01,\n",
       "         -1.4368e+00,  1.0673e+00,  9.2592e-01,  1.3570e+00,  3.1386e-01],\n",
       "        [-8.4153e-01,  3.8457e-01, -9.9795e-01, -6.9290e-01, -9.2146e-01,\n",
       "         -8.5886e-01, -9.0034e-01, -6.2294e-01,  7.7175e-01,  3.1930e-01],\n",
       "        [ 5.1308e-01, -5.1468e-01, -3.2251e-01,  4.4953e-01,  5.0860e-01,\n",
       "         -6.0778e-01,  4.9568e-01, -9.1306e-01,  5.6012e-02,  2.8025e-02],\n",
       "        [ 5.1547e-01, -3.0462e-01, -1.0122e+00,  3.5502e-01,  5.7070e-01,\n",
       "          6.5909e-02, -7.6933e-01,  2.9114e-01,  7.9156e-01, -2.7590e-01],\n",
       "        [-1.0591e+00, -8.7061e-01, -1.1684e+00,  6.9932e-01, -1.7733e-01,\n",
       "         -1.5826e+00, -2.4924e+00,  6.0663e-01,  3.9834e-01, -8.3086e-01],\n",
       "        [ 5.6302e-01,  3.5662e-01, -1.2813e+00, -1.0350e+00,  4.4525e-01,\n",
       "          3.8241e-01, -2.0672e+00, -4.1860e-01,  6.0244e-02, -7.0688e-01],\n",
       "        [ 7.7226e-01,  8.4145e-01,  9.7406e-01, -2.8393e-01,  1.7282e+00,\n",
       "         -5.6080e-01,  1.3265e+00, -1.7138e+00,  6.2380e-01, -5.3518e-01],\n",
       "        [ 2.8898e-01,  7.1464e-01,  2.3877e-01, -2.1722e-01, -9.2457e-01,\n",
       "          4.2064e-01,  1.0200e+00,  3.6816e-01,  6.5666e-01,  1.2068e+00],\n",
       "        [-8.2423e-01,  7.2873e-01, -4.9918e-01, -5.5348e-01, -2.7791e-01,\n",
       "          1.6756e+00,  8.0721e-02,  3.9490e-01, -1.4576e+00,  1.4843e+00],\n",
       "        [-1.9989e+00, -8.9610e-01,  7.7583e-01, -3.3385e-01,  8.7096e-02,\n",
       "          5.4479e-01,  2.1847e+00, -1.4134e+00, -8.4116e-01,  7.4556e-01],\n",
       "        [ 1.2091e+00,  1.3253e+00,  3.8431e-01,  2.2454e+00,  2.6141e-02,\n",
       "          1.3247e-01, -6.3037e-01, -5.0961e-01,  1.1369e+00, -1.1919e+00],\n",
       "        [ 4.8263e-02,  7.6555e-02, -1.3029e+00, -7.9430e-01,  6.7565e-01,\n",
       "          2.7272e+00,  8.7373e-01, -7.7285e-01,  9.5528e-01,  1.5974e+00],\n",
       "        [ 9.7565e-02, -3.9945e-01,  2.1727e-01, -1.0598e+00,  1.8235e-01,\n",
       "          9.2848e-01, -1.1630e+00,  1.7563e-01,  3.2870e-01,  1.9485e+00],\n",
       "        [-6.1082e-02, -9.0123e-01,  5.6985e-01,  3.1845e-01, -9.7500e-01,\n",
       "         -2.4546e-01, -4.8378e-01, -5.4943e-01, -1.1463e+00, -9.3503e-01],\n",
       "        [ 1.0236e+00,  2.2782e-01,  2.8463e-01, -3.3726e-01,  3.1493e-01,\n",
       "          4.7276e-01,  2.0334e+00, -1.6361e-01,  7.3497e-02,  9.4983e-01],\n",
       "        [ 9.7864e-02, -9.0554e-01,  1.1754e+00, -2.7798e-01,  6.4951e-02,\n",
       "          3.4921e-01, -4.0148e-01, -9.9058e-01, -1.8539e+00, -5.4846e-01],\n",
       "        [-4.6109e-01,  9.3092e-01,  1.5469e+00,  1.3755e+00,  7.3845e-01,\n",
       "         -1.3076e+00,  2.6296e-01,  6.6788e-01,  7.2389e-01, -1.2673e+00],\n",
       "        [ 5.9886e-01,  2.1078e+00, -4.9577e-02,  8.0892e-01, -2.8773e-02,\n",
       "         -3.3579e-01, -1.1821e+00,  2.0104e-03,  7.6610e-01, -5.7030e-01],\n",
       "        [-2.0743e+00,  1.2309e+00,  6.6796e-01,  8.3526e-01,  1.1301e-01,\n",
       "          5.5931e-01,  1.7886e+00,  7.8471e-01, -8.3463e-01,  3.0927e-01],\n",
       "        [-1.5890e-01, -4.8510e-01, -4.6081e-01,  6.4204e-01, -3.0351e-01,\n",
       "          9.3443e-01, -2.2268e+00, -2.2005e-01,  1.0009e+00, -3.5133e-01],\n",
       "        [-9.0050e-01,  2.3142e-01, -2.8167e-01,  5.9751e-01, -3.4383e-01,\n",
       "         -1.0612e+00, -3.3881e-01,  9.9113e-01,  1.9462e-01, -6.8564e-01],\n",
       "        [ 8.1261e-01, -1.9831e+00, -3.6409e-01, -1.4044e+00, -3.9022e-01,\n",
       "         -2.2721e-01,  2.1793e-01,  4.6646e-01,  2.9610e-01, -7.0583e-01],\n",
       "        [ 3.4614e-01,  8.1232e-01, -8.8059e-01, -1.4582e-01,  8.0655e-01,\n",
       "          2.3976e-01, -1.9529e-01, -2.8542e-01,  1.1039e+00, -5.1383e-01],\n",
       "        [ 1.1792e-01, -4.7168e-01, -1.1218e+00,  3.2226e+00, -1.7893e+00,\n",
       "         -7.6746e-01,  8.6243e-02, -8.6483e-01, -1.9056e+00,  1.5080e+00],\n",
       "        [ 1.7267e-01,  1.8689e+00, -4.5298e-01,  1.1790e+00,  1.0169e+00,\n",
       "          2.9224e-01, -1.8569e+00, -6.0096e-02,  1.6229e+00, -2.1187e+00],\n",
       "        [-7.4650e-01, -6.3267e-01, -7.4081e-01,  1.3137e+00, -3.2779e-01,\n",
       "         -2.1326e-01,  1.5954e+00, -1.7974e-01, -3.3108e-01, -6.5414e-01],\n",
       "        [ 3.3464e-01,  4.6817e-01,  9.6236e-02,  2.0823e-01, -5.9682e-01,\n",
       "         -1.5130e+00,  7.3971e-01, -6.4129e-01, -8.5583e-02,  1.6047e+00],\n",
       "        [-1.0909e+00, -2.2563e+00,  1.3378e+00,  1.9552e+00, -1.0091e+00,\n",
       "          7.5232e-01, -7.5363e-02,  3.9333e-01, -5.5577e-01, -1.0504e-01],\n",
       "        [ 5.1775e-01,  3.0980e-01,  2.9810e-01, -3.5973e-03, -1.4631e+00,\n",
       "          7.7358e-01,  2.0203e-01, -1.2333e+00, -4.0319e-01,  1.3088e-01],\n",
       "        [-2.7571e+00, -5.8823e-01, -1.0441e+00, -8.1000e-01, -1.0214e+00,\n",
       "          4.6769e-01,  2.8919e-01,  3.7736e-01, -7.3285e-01,  1.5595e+00],\n",
       "        [-6.0635e-01, -4.5618e-01,  1.6248e+00, -3.9644e-02, -4.2258e-01,\n",
       "         -2.1651e-01, -1.5503e-01, -8.8619e-01, -9.6481e-01,  1.0891e+00],\n",
       "        [ 3.3586e-01, -1.2208e+00, -9.8743e-02,  6.3612e-01, -1.6108e+00,\n",
       "         -1.5651e-01,  1.7886e+00,  6.8847e-02,  5.6431e-01, -7.0566e-01],\n",
       "        [ 1.2097e+00, -4.1157e-01, -1.6028e+00,  4.0233e-01,  9.7629e-01,\n",
       "         -9.5084e-01,  2.5246e+00, -8.2724e-01, -6.5388e-02, -1.0603e+00],\n",
       "        [ 6.5450e-01, -7.2573e-01, -1.1753e+00,  2.3776e-01,  2.8271e-01,\n",
       "          6.0272e-01, -1.7725e+00,  8.0077e-01, -3.4583e-02, -1.0340e+00],\n",
       "        [ 4.5506e-01,  2.7325e-01,  3.9985e-01,  1.8801e+00, -2.8098e-01,\n",
       "         -2.1624e+00,  1.0528e+00, -1.6296e+00, -3.7965e-01, -1.3353e-02],\n",
       "        [ 1.8694e+00, -1.6981e+00,  1.0309e-01, -3.2236e+00, -2.5016e-01,\n",
       "         -6.1043e-01, -9.6769e-01, -1.0162e-01, -5.6778e-01,  4.4146e-01],\n",
       "        [ 5.8712e-01,  5.3655e-02,  1.6432e-01,  1.6651e+00, -1.5942e+00,\n",
       "          5.3283e-01, -7.7235e-01,  1.2999e+00, -2.6921e-01, -1.1301e+00],\n",
       "        [-2.1125e+00, -1.2566e+00,  7.5240e-01, -4.4173e-01,  5.8324e-01,\n",
       "         -1.2734e-01,  7.4134e-01,  2.8895e-01,  1.7209e-02, -1.2599e+00],\n",
       "        [-2.3991e-01, -5.3501e-01, -1.1092e+00,  1.0544e+00,  6.6358e-01,\n",
       "         -1.9486e-01, -6.8351e-01, -4.6844e-01,  1.8849e-01,  2.8914e-01],\n",
       "        [-9.5989e-02,  2.5501e-01, -6.9189e-01, -8.6050e-01, -2.9190e+00,\n",
       "          5.4891e-03,  1.1012e+00, -1.9094e-01,  8.2894e-01, -7.4337e-01],\n",
       "        [-1.5048e-02,  1.7796e+00,  1.4521e+00,  9.5063e-01,  7.1683e-01,\n",
       "         -3.7837e-01, -1.1818e+00,  4.6409e-01,  1.8558e-01, -9.1750e-02],\n",
       "        [ 3.6800e-01, -2.3473e+00, -7.0824e-01, -1.9763e-01,  4.2292e-01,\n",
       "         -9.6419e-03, -5.8932e-01, -1.8057e+00,  1.6866e-01, -8.0163e-02],\n",
       "        [ 9.9156e-02, -1.6108e+00, -1.7687e+00, -1.1880e+00, -7.9637e-01,\n",
       "         -9.7600e-01, -1.9798e-01, -2.2316e-01, -2.1026e+00, -3.9070e-01],\n",
       "        [-1.0044e+00,  5.9778e-01, -3.4658e-01, -5.5261e-01, -2.0957e+00,\n",
       "         -2.0272e+00,  1.3110e+00, -1.2872e+00,  1.0368e+00,  1.2706e+00],\n",
       "        [-1.6162e+00, -4.9393e-01,  9.9052e-02,  1.7930e+00, -3.7346e-01,\n",
       "          1.6778e+00,  1.8596e+00,  5.6540e-01, -3.3038e-01,  1.3657e-01],\n",
       "        [-6.8283e-01, -1.3223e+00, -5.9822e-02, -9.7037e-01, -4.1225e-01,\n",
       "          1.6239e-01,  7.7454e-01,  1.1081e+00,  2.5188e+00,  9.3744e-01],\n",
       "        [-6.8269e-01, -2.5355e-01, -2.2029e+00,  1.7450e+00,  1.5713e+00,\n",
       "          2.8699e+00, -9.7431e-01, -1.6197e-01,  9.0702e-01,  7.1196e-01],\n",
       "        [-1.0708e+00,  1.0567e+00,  8.1746e-01, -2.2348e-01,  7.3921e-01,\n",
       "          4.4865e-01, -1.6067e+00, -8.8033e-01, -4.6711e-01,  1.8866e+00],\n",
       "        [-5.2908e-01,  6.1156e-01,  5.8678e-02,  8.4241e-01,  7.1553e-01,\n",
       "         -1.2238e+00,  1.7075e+00, -2.2461e-01, -1.1148e+00,  3.2187e-01],\n",
       "        [-1.8124e+00, -1.6278e+00, -1.3651e+00,  1.7115e-01,  6.7130e-01,\n",
       "          5.6318e-01,  2.3169e+00, -5.6681e-02,  1.6036e+00,  1.8284e+00],\n",
       "        [ 6.9168e-01,  1.1234e+00,  1.1111e+00,  2.2091e-01, -4.1417e-01,\n",
       "         -8.8011e-02,  3.0616e-01,  6.7661e-01, -7.4461e-01,  1.8952e-01],\n",
       "        [ 1.5691e+00,  1.1899e+00,  2.5851e+00, -5.1518e-01, -3.1629e-01,\n",
       "          1.1275e+00, -1.7460e+00,  2.5764e-01,  8.8055e-01,  2.3867e-01],\n",
       "        [-1.7332e+00,  1.1178e+00,  1.1068e+00,  1.0449e+00, -7.6628e-01,\n",
       "          4.2363e-01,  1.7011e+00, -9.1442e-01,  1.0713e+00,  5.4362e-01],\n",
       "        [ 7.6821e-01,  3.6046e-01,  2.7764e-01,  3.8973e-01,  5.1707e-01,\n",
       "          6.2875e-01, -1.2373e+00, -4.5715e-01, -8.3424e-02,  1.4036e+00],\n",
       "        [-2.2011e-01,  1.4555e+00,  1.5226e+00, -1.9717e-01,  2.4771e+00,\n",
       "         -2.1608e-01,  1.3705e+00, -4.0445e-01, -6.2426e-01,  8.7277e-01],\n",
       "        [ 8.0381e-01, -1.5532e+00, -1.2920e+00,  4.1829e-01, -8.3868e-01,\n",
       "          5.0812e-01,  5.0607e-01,  1.7789e+00,  4.9508e-01,  9.5733e-01],\n",
       "        [ 1.0914e-02,  2.5431e+00,  3.9935e-01, -6.8610e-01,  1.0956e+00,\n",
       "          8.5874e-01,  7.6796e-01, -1.2574e+00,  2.3066e-01,  5.1398e-01],\n",
       "        [-1.4865e+00, -1.5451e-01,  4.7349e-01, -4.0788e-01,  1.6721e+00,\n",
       "         -2.9521e-01,  9.5256e-01, -1.3572e-01, -1.6855e-01, -1.1286e+00],\n",
       "        [-1.7098e+00, -1.5272e-01, -7.8794e-01,  1.8199e+00, -8.0728e-01,\n",
       "         -3.8703e-01,  2.1660e+00, -6.0556e-01,  1.8203e+00, -4.0219e-03],\n",
       "        [ 9.1467e-01, -1.5138e-01, -3.0147e-01,  1.2895e+00, -6.0340e-01,\n",
       "         -1.6142e-01,  5.7665e-01, -8.1516e-01,  7.3980e-01,  5.5937e-01],\n",
       "        [-2.2169e-01,  7.9586e-01, -1.5912e+00,  6.8667e-02,  6.8934e-01,\n",
       "         -8.2136e-01,  1.1088e+00, -3.4797e-01, -2.2308e-03,  4.9656e-01],\n",
       "        [-1.6740e-01, -9.1980e-01,  2.1272e+00,  1.0887e+00,  4.2608e-01,\n",
       "          1.2440e+00,  1.5343e+00, -1.0174e+00,  4.8286e-01,  6.7247e-03],\n",
       "        [ 3.1484e-01, -8.6978e-01, -3.9800e-01,  2.8481e-02,  5.1683e-01,\n",
       "          1.4591e+00, -8.9912e-03,  1.2585e+00,  1.3745e-01,  8.3999e-01],\n",
       "        [ 1.7364e+00,  8.6295e-01, -8.7128e-01,  6.8410e-01,  1.5289e+00,\n",
       "          1.0446e-01,  1.5353e-01, -6.4065e-01,  1.5511e+00,  5.8256e-01],\n",
       "        [-8.9996e-02,  2.0217e-01,  1.4883e+00,  1.3878e+00,  1.7805e-02,\n",
       "          1.4642e+00, -1.7292e+00, -4.6524e-01, -4.3653e-01,  2.9926e-01],\n",
       "        [-2.3165e+00, -1.5633e+00,  9.8927e-01, -5.7441e-01, -6.6910e-02,\n",
       "         -6.3237e-01,  4.4890e-01,  2.7030e-01,  3.9487e-02, -7.7512e-01],\n",
       "        [-1.2969e+00,  6.1197e-01,  2.2034e-02, -7.2825e-03,  2.9932e-01,\n",
       "         -7.4145e-01,  1.2599e+00, -1.4514e+00,  3.4560e-01,  3.1058e-01],\n",
       "        [-3.2898e-01, -5.7605e-01, -4.8897e-01, -3.9571e-01, -8.4078e-01,\n",
       "          2.6832e-01,  1.2990e+00, -8.0536e-01,  8.4911e-01,  4.3493e-01],\n",
       "        [ 1.9641e+00,  5.0395e-04,  1.1141e+00,  1.1864e-01, -1.7620e-01,\n",
       "          1.0683e+00, -3.6807e-02, -6.1221e-01,  1.0119e+00, -6.6408e-02],\n",
       "        [ 1.4004e+00, -9.7094e-01, -6.6857e-01,  8.4786e-01,  8.7334e-01,\n",
       "          9.0857e-02, -4.9365e-01, -1.9715e-01, -5.6527e-01, -4.9225e-01],\n",
       "        [-2.9695e-01, -1.2707e+00, -2.5387e+00,  1.4624e+00, -1.5575e+00,\n",
       "         -6.4354e-01,  1.6666e+00, -3.4293e-01,  4.6894e-01,  9.5140e-01],\n",
       "        [ 2.1762e-01, -6.1584e-01,  6.1603e-01, -2.5760e-01,  6.3342e-01,\n",
       "          1.6923e+00,  8.1893e-01,  1.2560e+00,  2.9152e-01, -2.2735e+00],\n",
       "        [-1.2221e+00, -1.4899e-01,  1.6798e+00, -2.9654e-01, -2.5054e+00,\n",
       "          1.1928e-01,  6.2393e-01,  3.0447e+00, -1.6104e+00,  1.3546e-01],\n",
       "        [ 9.4122e-01,  2.1211e-01,  1.5687e-01,  1.2768e+00,  1.2704e+00,\n",
       "         -2.9695e-01, -1.3322e+00, -2.6048e-01,  2.8403e-01,  3.1739e-01],\n",
       "        [ 1.2084e+00, -1.3470e-01,  7.1102e-01,  1.0147e+00, -1.1499e-01,\n",
       "         -1.7931e+00,  2.6357e-01,  5.5427e-01, -4.5920e-01,  9.4827e-01]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_initial_weights = model.user_factors.weight.detach()\n",
    "user_initial_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this is required for the ``fork`` method to work\n",
    "model.share_memory()\n",
    "processes = []\n",
    "for rank in range(num_processes):\n",
    "    p = mp.Process(target=train, args=(model, ))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4333e+00, -8.9963e-01, -1.9330e-02, -1.7814e+00,  2.0324e-02,\n",
       "         -4.1041e-01,  1.4695e+00, -2.3004e+00, -5.5006e-01,  9.6585e-01],\n",
       "        [-1.4763e+00, -4.5092e-01,  9.0537e-01, -1.7011e+00,  1.7471e+00,\n",
       "          1.2400e+00,  7.6806e-01, -7.3178e-01,  1.9154e-01, -1.6154e+00],\n",
       "        [ 3.8457e-01, -6.9640e-01,  4.4247e-01,  8.9063e-01,  7.2175e-01,\n",
       "          1.5651e-01,  5.6657e-01, -6.5262e-01,  5.3221e-01, -1.0787e+00],\n",
       "        [-9.5290e-01,  3.6351e-02,  8.9709e-01,  2.3399e-01,  6.7542e-01,\n",
       "         -8.4097e-01,  9.9748e-01, -1.3285e+00, -3.4416e-02,  5.0887e-01],\n",
       "        [ 5.9784e-01, -9.0697e-01, -7.1282e-01, -2.4948e-01, -6.4971e-01,\n",
       "          8.7206e-01, -5.8542e-01, -3.2919e-01, -3.4160e-01,  1.3576e-01],\n",
       "        [-1.9971e-01,  1.5334e+00,  1.3532e+00,  1.1532e+00, -1.7517e+00,\n",
       "         -8.9848e-01, -5.7436e-01,  2.4249e-01,  1.1253e+00,  1.1115e+00],\n",
       "        [-1.9362e-01,  1.7086e+00,  2.7931e-01,  5.5493e-01, -9.6455e-01,\n",
       "         -1.7386e+00,  2.4911e-01, -1.5715e+00,  3.8965e-01,  1.3052e+00],\n",
       "        [-9.3871e-01,  1.5376e-01, -1.0461e+00,  9.5612e-01, -1.6406e+00,\n",
       "         -3.6505e-01,  3.5504e-01,  1.7138e+00, -5.2552e-01,  5.3002e-02],\n",
       "        [-2.3685e+00, -1.7228e+00, -5.6088e-01,  2.4156e-01,  3.3708e-02,\n",
       "         -6.8205e-01,  5.4853e-01,  2.3913e-01, -3.7969e-02, -2.3545e-01],\n",
       "        [-2.2254e-01, -8.5967e-01,  2.8247e-01, -1.0681e+00, -8.9280e-01,\n",
       "         -8.1529e-01, -1.4039e-01,  2.2843e-01, -2.2449e+00, -1.9916e+00],\n",
       "        [ 2.7216e-01, -1.0698e+00, -1.1813e+00, -1.6923e+00,  1.2578e+00,\n",
       "          1.6030e+00, -4.9662e-01,  1.2040e-01, -1.0337e+00,  4.3306e-01],\n",
       "        [-2.7639e-01,  3.8737e-01,  9.5247e-02, -1.1213e+00, -7.0410e-01,\n",
       "          2.9937e-01,  1.4210e+00, -4.2727e-01, -2.2150e+00, -1.2833e+00],\n",
       "        [-1.3831e-01,  1.6157e+00,  7.6762e-01,  2.2031e-01,  5.2692e-01,\n",
       "          5.8650e-01,  3.7986e-01,  2.3556e-01, -2.1529e-01,  9.6105e-01],\n",
       "        [-6.1858e-01,  1.6063e+00, -2.7682e-01,  5.5596e-01, -3.5311e-02,\n",
       "          1.0390e+00,  1.2079e-01,  2.3063e+00,  1.4701e+00, -1.9631e+00],\n",
       "        [-1.0903e+00,  1.9322e-04,  5.3116e-02,  1.9153e+00,  1.0844e+00,\n",
       "          4.8793e-01,  1.1994e+00,  5.9839e-02,  9.3380e-01,  6.5793e-01],\n",
       "        [ 1.4146e+00, -7.4532e-01, -1.9150e-01,  1.4040e-01,  3.6602e-01,\n",
       "         -1.4368e+00,  1.0673e+00,  9.2592e-01,  1.3570e+00,  3.1386e-01],\n",
       "        [-8.4153e-01,  3.8457e-01, -9.9795e-01, -6.9290e-01, -9.2146e-01,\n",
       "         -8.5886e-01, -9.0034e-01, -6.2294e-01,  7.7175e-01,  3.1930e-01],\n",
       "        [ 5.1308e-01, -5.1468e-01, -3.2251e-01,  4.4953e-01,  5.0860e-01,\n",
       "         -6.0778e-01,  4.9568e-01, -9.1306e-01,  5.6011e-02,  2.8025e-02],\n",
       "        [ 5.1547e-01, -3.0462e-01, -1.0122e+00,  3.5502e-01,  5.7070e-01,\n",
       "          6.5909e-02, -7.6933e-01,  2.9114e-01,  7.9156e-01, -2.7590e-01],\n",
       "        [-1.0591e+00, -8.7061e-01, -1.1684e+00,  6.9932e-01, -1.7733e-01,\n",
       "         -1.5826e+00, -2.4924e+00,  6.0663e-01,  3.9834e-01, -8.3086e-01],\n",
       "        [ 5.6302e-01,  3.5662e-01, -1.2813e+00, -1.0350e+00,  4.4525e-01,\n",
       "          3.8241e-01, -2.0672e+00, -4.1860e-01,  6.0244e-02, -7.0688e-01],\n",
       "        [ 7.7226e-01,  8.4145e-01,  9.7406e-01, -2.8393e-01,  1.7282e+00,\n",
       "         -5.6080e-01,  1.3265e+00, -1.7138e+00,  6.2380e-01, -5.3518e-01],\n",
       "        [ 2.8898e-01,  7.1464e-01,  2.3877e-01, -2.1722e-01, -9.2457e-01,\n",
       "          4.2064e-01,  1.0200e+00,  3.6816e-01,  6.5666e-01,  1.2068e+00],\n",
       "        [-8.2423e-01,  7.2873e-01, -4.9918e-01, -5.5348e-01, -2.7791e-01,\n",
       "          1.6756e+00,  8.0721e-02,  3.9490e-01, -1.4576e+00,  1.4843e+00],\n",
       "        [-1.9989e+00, -8.9610e-01,  7.7583e-01, -3.3385e-01,  8.7096e-02,\n",
       "          5.4479e-01,  2.1847e+00, -1.4134e+00, -8.4116e-01,  7.4556e-01],\n",
       "        [ 1.2091e+00,  1.3253e+00,  3.8431e-01,  2.2454e+00,  2.6140e-02,\n",
       "          1.3247e-01, -6.3037e-01, -5.0961e-01,  1.1369e+00, -1.1919e+00],\n",
       "        [ 4.8263e-02,  7.6555e-02, -1.3029e+00, -7.9430e-01,  6.7565e-01,\n",
       "          2.7272e+00,  8.7373e-01, -7.7285e-01,  9.5528e-01,  1.5974e+00],\n",
       "        [ 9.7565e-02, -3.9945e-01,  2.1727e-01, -1.0598e+00,  1.8235e-01,\n",
       "          9.2848e-01, -1.1630e+00,  1.7562e-01,  3.2870e-01,  1.9485e+00],\n",
       "        [-6.1083e-02, -9.0123e-01,  5.6985e-01,  3.1845e-01, -9.7500e-01,\n",
       "         -2.4546e-01, -4.8378e-01, -5.4943e-01, -1.1463e+00, -9.3503e-01],\n",
       "        [ 1.0236e+00,  2.2782e-01,  2.8463e-01, -3.3726e-01,  3.1493e-01,\n",
       "          4.7276e-01,  2.0334e+00, -1.6361e-01,  7.3497e-02,  9.4983e-01],\n",
       "        [ 9.7864e-02, -9.0554e-01,  1.1754e+00, -2.7798e-01,  6.4951e-02,\n",
       "          3.4921e-01, -4.0148e-01, -9.9058e-01, -1.8539e+00, -5.4846e-01],\n",
       "        [-4.6109e-01,  9.3092e-01,  1.5469e+00,  1.3755e+00,  7.3845e-01,\n",
       "         -1.3076e+00,  2.6296e-01,  6.6788e-01,  7.2389e-01, -1.2673e+00],\n",
       "        [ 5.9886e-01,  2.1078e+00, -4.9577e-02,  8.0892e-01, -2.8773e-02,\n",
       "         -3.3579e-01, -1.1821e+00,  2.0104e-03,  7.6610e-01, -5.7030e-01],\n",
       "        [-2.0743e+00,  1.2309e+00,  6.6796e-01,  8.3526e-01,  1.1301e-01,\n",
       "          5.5931e-01,  1.7886e+00,  7.8471e-01, -8.3463e-01,  3.0927e-01],\n",
       "        [-1.5890e-01, -4.8510e-01, -4.6081e-01,  6.4204e-01, -3.0351e-01,\n",
       "          9.3443e-01, -2.2268e+00, -2.2005e-01,  1.0009e+00, -3.5133e-01],\n",
       "        [-9.0050e-01,  2.3142e-01, -2.8167e-01,  5.9751e-01, -3.4383e-01,\n",
       "         -1.0612e+00, -3.3881e-01,  9.9113e-01,  1.9462e-01, -6.8564e-01],\n",
       "        [ 8.1261e-01, -1.9831e+00, -3.6409e-01, -1.4044e+00, -3.9022e-01,\n",
       "         -2.2721e-01,  2.1793e-01,  4.6646e-01,  2.9610e-01, -7.0583e-01],\n",
       "        [ 3.4614e-01,  8.1232e-01, -8.8059e-01, -1.4582e-01,  8.0655e-01,\n",
       "          2.3976e-01, -1.9529e-01, -2.8542e-01,  1.1039e+00, -5.1383e-01],\n",
       "        [ 1.1792e-01, -4.7168e-01, -1.1218e+00,  3.2226e+00, -1.7893e+00,\n",
       "         -7.6746e-01,  8.6244e-02, -8.6483e-01, -1.9056e+00,  1.5080e+00],\n",
       "        [ 1.7267e-01,  1.8689e+00, -4.5298e-01,  1.1790e+00,  1.0169e+00,\n",
       "          2.9224e-01, -1.8569e+00, -6.0096e-02,  1.6229e+00, -2.1187e+00],\n",
       "        [-7.4650e-01, -6.3267e-01, -7.4081e-01,  1.3137e+00, -3.2779e-01,\n",
       "         -2.1326e-01,  1.5954e+00, -1.7974e-01, -3.3108e-01, -6.5414e-01],\n",
       "        [ 3.3464e-01,  4.6817e-01,  9.6235e-02,  2.0823e-01, -5.9682e-01,\n",
       "         -1.5130e+00,  7.3971e-01, -6.4129e-01, -8.5582e-02,  1.6047e+00],\n",
       "        [-1.0909e+00, -2.2563e+00,  1.3378e+00,  1.9552e+00, -1.0091e+00,\n",
       "          7.5232e-01, -7.5363e-02,  3.9333e-01, -5.5577e-01, -1.0504e-01],\n",
       "        [ 5.1775e-01,  3.0980e-01,  2.9810e-01, -3.5970e-03, -1.4631e+00,\n",
       "          7.7358e-01,  2.0203e-01, -1.2333e+00, -4.0319e-01,  1.3088e-01],\n",
       "        [-2.7571e+00, -5.8823e-01, -1.0441e+00, -8.1000e-01, -1.0214e+00,\n",
       "          4.6769e-01,  2.8919e-01,  3.7736e-01, -7.3285e-01,  1.5595e+00],\n",
       "        [-6.0635e-01, -4.5618e-01,  1.6248e+00, -3.9644e-02, -4.2258e-01,\n",
       "         -2.1651e-01, -1.5503e-01, -8.8619e-01, -9.6481e-01,  1.0891e+00],\n",
       "        [ 3.3586e-01, -1.2208e+00, -9.8743e-02,  6.3612e-01, -1.6108e+00,\n",
       "         -1.5651e-01,  1.7886e+00,  6.8847e-02,  5.6431e-01, -7.0566e-01],\n",
       "        [ 1.2097e+00, -4.1157e-01, -1.6028e+00,  4.0233e-01,  9.7629e-01,\n",
       "         -9.5084e-01,  2.5246e+00, -8.2724e-01, -6.5388e-02, -1.0603e+00],\n",
       "        [ 6.5450e-01, -7.2573e-01, -1.1753e+00,  2.3776e-01,  2.8271e-01,\n",
       "          6.0272e-01, -1.7725e+00,  8.0077e-01, -3.4583e-02, -1.0340e+00],\n",
       "        [ 4.5506e-01,  2.7325e-01,  3.9985e-01,  1.8801e+00, -2.8098e-01,\n",
       "         -2.1624e+00,  1.0528e+00, -1.6296e+00, -3.7965e-01, -1.3352e-02],\n",
       "        [ 1.8694e+00, -1.6981e+00,  1.0309e-01, -3.2236e+00, -2.5016e-01,\n",
       "         -6.1043e-01, -9.6769e-01, -1.0162e-01, -5.6778e-01,  4.4146e-01],\n",
       "        [ 5.8712e-01,  5.3655e-02,  1.6432e-01,  1.6651e+00, -1.5942e+00,\n",
       "          5.3283e-01, -7.7235e-01,  1.2999e+00, -2.6921e-01, -1.1301e+00],\n",
       "        [-2.1125e+00, -1.2566e+00,  7.5240e-01, -4.4173e-01,  5.8324e-01,\n",
       "         -1.2734e-01,  7.4134e-01,  2.8895e-01,  1.7208e-02, -1.2599e+00],\n",
       "        [-2.3991e-01, -5.3501e-01, -1.1092e+00,  1.0544e+00,  6.6358e-01,\n",
       "         -1.9486e-01, -6.8351e-01, -4.6844e-01,  1.8849e-01,  2.8914e-01],\n",
       "        [-9.5989e-02,  2.5501e-01, -6.9189e-01, -8.6050e-01, -2.9190e+00,\n",
       "          5.4890e-03,  1.1012e+00, -1.9094e-01,  8.2894e-01, -7.4337e-01],\n",
       "        [-1.5048e-02,  1.7796e+00,  1.4521e+00,  9.5063e-01,  7.1683e-01,\n",
       "         -3.7837e-01, -1.1818e+00,  4.6409e-01,  1.8558e-01, -9.1750e-02],\n",
       "        [ 3.6800e-01, -2.3473e+00, -7.0824e-01, -1.9763e-01,  4.2292e-01,\n",
       "         -9.6419e-03, -5.8932e-01, -1.8057e+00,  1.6866e-01, -8.0163e-02],\n",
       "        [ 9.9156e-02, -1.6108e+00, -1.7687e+00, -1.1880e+00, -7.9637e-01,\n",
       "         -9.7600e-01, -1.9798e-01, -2.2316e-01, -2.1026e+00, -3.9070e-01],\n",
       "        [-1.0044e+00,  5.9778e-01, -3.4658e-01, -5.5261e-01, -2.0957e+00,\n",
       "         -2.0272e+00,  1.3110e+00, -1.2872e+00,  1.0368e+00,  1.2706e+00],\n",
       "        [-1.6162e+00, -4.9393e-01,  9.9052e-02,  1.7930e+00, -3.7346e-01,\n",
       "          1.6778e+00,  1.8596e+00,  5.6540e-01, -3.3038e-01,  1.3657e-01],\n",
       "        [-6.8283e-01, -1.3223e+00, -5.9822e-02, -9.7037e-01, -4.1225e-01,\n",
       "          1.6239e-01,  7.7454e-01,  1.1081e+00,  2.5188e+00,  9.3744e-01],\n",
       "        [-6.8269e-01, -2.5355e-01, -2.2029e+00,  1.7450e+00,  1.5713e+00,\n",
       "          2.8699e+00, -9.7431e-01, -1.6197e-01,  9.0702e-01,  7.1196e-01],\n",
       "        [-1.0708e+00,  1.0567e+00,  8.1746e-01, -2.2348e-01,  7.3921e-01,\n",
       "          4.4865e-01, -1.6067e+00, -8.8033e-01, -4.6711e-01,  1.8866e+00],\n",
       "        [-5.2908e-01,  6.1156e-01,  5.8678e-02,  8.4241e-01,  7.1553e-01,\n",
       "         -1.2238e+00,  1.7075e+00, -2.2461e-01, -1.1148e+00,  3.2187e-01],\n",
       "        [-1.8124e+00, -1.6278e+00, -1.3651e+00,  1.7115e-01,  6.7130e-01,\n",
       "          5.6318e-01,  2.3169e+00, -5.6681e-02,  1.6036e+00,  1.8284e+00],\n",
       "        [ 6.9168e-01,  1.1234e+00,  1.1111e+00,  2.2091e-01, -4.1417e-01,\n",
       "         -8.8011e-02,  3.0616e-01,  6.7661e-01, -7.4461e-01,  1.8952e-01],\n",
       "        [ 1.5691e+00,  1.1899e+00,  2.5851e+00, -5.1518e-01, -3.1629e-01,\n",
       "          1.1275e+00, -1.7460e+00,  2.5764e-01,  8.8055e-01,  2.3867e-01],\n",
       "        [-1.7332e+00,  1.1178e+00,  1.1068e+00,  1.0449e+00, -7.6628e-01,\n",
       "          4.2363e-01,  1.7011e+00, -9.1442e-01,  1.0713e+00,  5.4362e-01],\n",
       "        [ 7.6821e-01,  3.6046e-01,  2.7764e-01,  3.8973e-01,  5.1707e-01,\n",
       "          6.2875e-01, -1.2373e+00, -4.5715e-01, -8.3424e-02,  1.4036e+00],\n",
       "        [-2.2011e-01,  1.4555e+00,  1.5226e+00, -1.9717e-01,  2.4771e+00,\n",
       "         -2.1608e-01,  1.3705e+00, -4.0445e-01, -6.2426e-01,  8.7277e-01],\n",
       "        [ 8.0381e-01, -1.5532e+00, -1.2920e+00,  4.1829e-01, -8.3868e-01,\n",
       "          5.0812e-01,  5.0607e-01,  1.7789e+00,  4.9508e-01,  9.5733e-01],\n",
       "        [ 1.0914e-02,  2.5431e+00,  3.9935e-01, -6.8610e-01,  1.0956e+00,\n",
       "          8.5874e-01,  7.6796e-01, -1.2574e+00,  2.3066e-01,  5.1398e-01],\n",
       "        [-1.4865e+00, -1.5451e-01,  4.7349e-01, -4.0788e-01,  1.6721e+00,\n",
       "         -2.9521e-01,  9.5256e-01, -1.3572e-01, -1.6855e-01, -1.1286e+00],\n",
       "        [-1.7098e+00, -1.5272e-01, -7.8794e-01,  1.8199e+00, -8.0728e-01,\n",
       "         -3.8703e-01,  2.1660e+00, -6.0556e-01,  1.8203e+00, -4.0220e-03],\n",
       "        [ 9.1467e-01, -1.5138e-01, -3.0147e-01,  1.2895e+00, -6.0340e-01,\n",
       "         -1.6142e-01,  5.7665e-01, -8.1516e-01,  7.3980e-01,  5.5937e-01],\n",
       "        [-2.2169e-01,  7.9586e-01, -1.5912e+00,  6.8667e-02,  6.8934e-01,\n",
       "         -8.2136e-01,  1.1088e+00, -3.4797e-01, -2.2306e-03,  4.9656e-01],\n",
       "        [-1.6740e-01, -9.1980e-01,  2.1272e+00,  1.0887e+00,  4.2608e-01,\n",
       "          1.2440e+00,  1.5343e+00, -1.0174e+00,  4.8286e-01,  6.7249e-03],\n",
       "        [ 3.1484e-01, -8.6978e-01, -3.9800e-01,  2.8481e-02,  5.1683e-01,\n",
       "          1.4591e+00, -8.9909e-03,  1.2585e+00,  1.3745e-01,  8.3999e-01],\n",
       "        [ 1.7364e+00,  8.6295e-01, -8.7128e-01,  6.8410e-01,  1.5289e+00,\n",
       "          1.0446e-01,  1.5353e-01, -6.4065e-01,  1.5511e+00,  5.8256e-01],\n",
       "        [-8.9996e-02,  2.0217e-01,  1.4883e+00,  1.3878e+00,  1.7805e-02,\n",
       "          1.4642e+00, -1.7292e+00, -4.6524e-01, -4.3653e-01,  2.9926e-01],\n",
       "        [-2.3165e+00, -1.5633e+00,  9.8927e-01, -5.7441e-01, -6.6910e-02,\n",
       "         -6.3237e-01,  4.4890e-01,  2.7030e-01,  3.9487e-02, -7.7512e-01],\n",
       "        [-1.2969e+00,  6.1197e-01,  2.2034e-02, -7.2825e-03,  2.9932e-01,\n",
       "         -7.4145e-01,  1.2599e+00, -1.4514e+00,  3.4560e-01,  3.1058e-01],\n",
       "        [-3.2898e-01, -5.7605e-01, -4.8897e-01, -3.9571e-01, -8.4078e-01,\n",
       "          2.6832e-01,  1.2990e+00, -8.0536e-01,  8.4911e-01,  4.3493e-01],\n",
       "        [ 1.9641e+00,  5.0395e-04,  1.1141e+00,  1.1864e-01, -1.7620e-01,\n",
       "          1.0683e+00, -3.6807e-02, -6.1221e-01,  1.0119e+00, -6.6408e-02],\n",
       "        [ 1.4004e+00, -9.7094e-01, -6.6857e-01,  8.4786e-01,  8.7334e-01,\n",
       "          9.0857e-02, -4.9365e-01, -1.9715e-01, -5.6527e-01, -4.9225e-01],\n",
       "        [-2.9695e-01, -1.2707e+00, -2.5387e+00,  1.4624e+00, -1.5575e+00,\n",
       "         -6.4354e-01,  1.6666e+00, -3.4293e-01,  4.6894e-01,  9.5140e-01],\n",
       "        [ 2.1762e-01, -6.1584e-01,  6.1603e-01, -2.5760e-01,  6.3342e-01,\n",
       "          1.6923e+00,  8.1893e-01,  1.2560e+00,  2.9152e-01, -2.2735e+00],\n",
       "        [-1.2221e+00, -1.4899e-01,  1.6798e+00, -2.9654e-01, -2.5054e+00,\n",
       "          1.1928e-01,  6.2393e-01,  3.0447e+00, -1.6104e+00,  1.3546e-01],\n",
       "        [ 9.4122e-01,  2.1211e-01,  1.5687e-01,  1.2768e+00,  1.2704e+00,\n",
       "         -2.9695e-01, -1.3322e+00, -2.6048e-01,  2.8403e-01,  3.1739e-01],\n",
       "        [ 1.2084e+00, -1.3470e-01,  7.1102e-01,  1.0147e+00, -1.1499e-01,\n",
       "         -1.7931e+00,  2.6357e-01,  5.5427e-01, -4.5920e-01,  9.4827e-01]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_final_weights = model.user_factors.weight.detach()\n",
    "user_final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_initial_weights == user_final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_item_for_user(model, user_id):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    m = model.eval().cpu()\n",
    "\n",
    "    user_ids = torch.LongTensor([user2idx[u] for u in [user_id] * len(items)])\n",
    "    item_ids = torch.LongTensor([item2idx[b] for b in items])\n",
    "\n",
    "    remove = set(ratings[ratings[user_col] == user_id][item_col].values)\n",
    "\n",
    "    preds = m(user_ids, item_ids).detach().numpy()\n",
    "    pred_item = [\n",
    "        (p, b) for p, b in sorted(zip(preds, items), reverse=True) if b not in remove\n",
    "    ]\n",
    "\n",
    "    return pred_item\n",
    "\n",
    "\n",
    "def recommend_user_for_item(model, item_id):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    m = model.eval().cpu()\n",
    "\n",
    "    user_ids = torch.LongTensor([user2idx[u] for u in users])\n",
    "    book_ids = torch.LongTensor([item2idx[b] for b in [item_id] * len(users)])\n",
    "\n",
    "    remove = set(ratings[ratings[item_col] == book_id][user_col].values)\n",
    "\n",
    "    preds = m(user_ids, item_ids).detach().numpy()\n",
    "    pred_user = [\n",
    "        (p, u) for p, u in sorted(zip(preds, users), reverse=True) if u not in remove\n",
    "    ]\n",
    "\n",
    "    return pred_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
