# Copyright (c) 2019, Corey Smith
# Distributed under the MIT License.
# See LICENCE file in root directory for full terms.
"""
Neural network matrix factorization library.

"""
import torch
import torch.nn as nn


class ScaledEmbedding(nn.Embedding):
    """
    Embedding layer that initialises its values with a normal distribution.
    The variance is set to the inverse of the embedding dimension squared.

    .. math:: weights ~ N(0, (\frac{1}{n_d})^2)
    """

    def reset_parameters(self):
        """
        Initialize parameters.
        """
        self.weight.data.normal_(0, 1.0 / self.embedding_dim)


class ZeroEmbedding(nn.Embedding):
    """
    Embedding layer that initialises its values
    to zero. Used for biases.
    """

    def reset_parameters(self):
        """
        Initialize parameters.
        """
        self.weight.data.zero_()


class MatrixFactorization(torch.nn.Module):
    """Matrix factorization using pytorch."""

    def __init__(
        self,
        n_users,
        n_products,
        n_factors=20,
        optimizer=torch.optim.SGD,
        lr=0.001,
        l2=0,
        momentum=0,
        loss_fn=nn.BCEWithLogitsLoss,
        activation=nn.Sigmoid,
    ):
        """
        Initalize the user and product embedding vectors in latent space.

        Args:
            n_users (int): Number of users with prior purchases.
            n_products (int): Total number of products purchased.
            n_factors (integer, optional): Dimension of the latent embedding space.
        """
        super(MatrixFactorization, self).__init__()

        self.l2 = l2
        self.lr = lr
        self.momentum = momentum
        self.user_factors = ScaledEmbedding(n_users, n_factors)
        self.product_factors = ScaledEmbedding(n_products, n_factors)
        self.user_bias = ZeroEmbedding(n_users, 1)
        self.product_bias = ZeroEmbedding(n_products, 1)

        self.activation = activation()
        self.loss_fn = loss_fn()
        self.optimizer = optimizer(
            self.parameters(), lr=self.lr, weight_decay=self.l2, momentum=self.momentum
        )
        self.grad = None

    def forward(self, user, item):
        """
        Matrix multiplication between user and product
        embedding vectors.
        """
        mat_mult = (self.user_bias(user) + self.product_bias(item)).squeeze(2)
        mat_mult += ((self.user_factors(user)) * (self.product_factors(item))).sum(2)

        return mat_mult

    def _prob_to_class(self, forward):
        """
        Convert the probabilities from the final activation into a
        binary classification.
        """
        predict_pos = self.activation(forward)
        predict_neg = 1 - predict_pos

        return torch.stack((predict_neg, predict_pos)).argmax(0).float()

    def prediction(self, user, item):
        """
        Use product and user embedding vectors to calculate
        a probability for positive interaction.
        """
        return self._prob_to_class(self(user, item))

    def loss(self, forward, rating):
        """Calculate the loss of the predicted ratings."""
        return self.loss_fn(forward, rating.float())

    def compute_accuracy(self, data_loader):
        """
        Compute the accuracy of our predictions against the true ratings.
        """
        correct = 0
        total = 0

        self.eval()
        with torch.no_grad():
            for user, item, true_rating in data_loader:
                predicted = self.prediction(user, item)
                total += predicted.numel()
                correct += (predicted == true_rating).sum().item()

        return total, correct

    def train_model(self, data_loader):
        """
        Train the model on the data generated by the dataloader and compute
        the training loss and training accuracy.
        """
        train_loss = 0
        correct = 0
        total = 0

        self.train()
        for user, item, rating in data_loader:
            self.optimizer.zero_grad()

            forward = self(user, item)
            predicted = self._prob_to_class(forward)
            loss = self.loss(forward, rating)

            train_loss += loss.item()
            total += predicted.numel()
            correct += (predicted == rating).sum().item()

            loss.backward()
            self.optimizer.step()

        return train_loss, f"{(100 * correct / total):.2f}"

    def evaluate(self, dataloader):
        """
        Calculate the loss and accuracy of the model on the validation
        or test data set.
        """
        val_loss = 0
        correct = 0
        total = 0

        self.eval()
        with torch.no_grad():
            for user, item, rating in dataloader:
                forward = self(user, item)
                predicted = self._prob_to_class(forward)

                val_loss += self.loss(forward, rating).item()
                total += predicted.numel()
                correct += (predicted == rating).sum().item()

        return val_loss, f"{(100 * correct / total):.2f}"
